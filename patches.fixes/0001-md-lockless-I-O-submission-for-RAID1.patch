From: Hannes Reinecke <hare@suse.de>
Date: Wed, 27 Jul 2016 08:43:15 +0200
Subject: [PATCH] md: lockless I/O submission for RAID1
Patch-mainline: Not yet, this patch is specific for SLE11-SP4
Reference: bsc#982783

We should avoid taking a lock in the hotpath during I/O submission;
this hurts performance very bad when running on fast storage.

(Rebased by Coly Li for SLE12-SP1)

Signed-off-by: Hannes Reinecke <hare@suse.com>
Signed-off-by: Coly Li <colyli@suse.de>
---
 drivers/md/raid1.c |   93 ++++++++++++++++++++++++++---------------------------
 drivers/md/raid1.h |   13 +++----
 2 files changed, 53 insertions(+), 53 deletions(-)

--- a/drivers/md/raid1.c
+++ b/drivers/md/raid1.c
@@ -866,16 +866,18 @@ static void raise_barrier(struct r1conf
 	spin_lock_irq(&conf->resync_lock);
 
 	/* Wait until no block IO is waiting */
-	wait_event_lock_irq(conf->wait_barrier, !conf->nr_waiting,
+	wait_event_lock_irq(conf->wait_barrier,
+			    !atomic_read(&conf->nr_waiting),
 			    conf->resync_lock);
 
 	/* block any new IO from starting */
-	conf->barrier++;
+	atomic_inc(&conf->barrier);
 
 	/* Now wait for all pending IO to complete */
 	wait_event_lock_irq(conf->wait_barrier,
-			    !conf->array_frozen &&
-			    !conf->nr_pending && conf->barrier < RESYNC_DEPTH,
+			    !test_bit(R1FLAG_ARRAY_FROZEN, &conf->flags) &&
+			    !atomic_read(&conf->nr_pending) &&
+			    atomic_read(&conf->barrier) < RESYNC_DEPTH,
 			    conf->resync_lock);
 
 	spin_unlock_irq(&conf->resync_lock);
@@ -883,19 +885,18 @@ static void raise_barrier(struct r1conf
 
 static void lower_barrier(struct r1conf *conf)
 {
-	unsigned long flags;
-	BUG_ON(conf->barrier <= 0);
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->barrier--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	BUG_ON(atomic_read(&conf->barrier) <= 0);
+	atomic_dec(&conf->barrier);
 	wake_up(&conf->wait_barrier);
 }
 
 static void wait_barrier(struct r1conf *conf)
 {
-	spin_lock_irq(&conf->resync_lock);
-	if (conf->barrier) {
-		conf->nr_waiting++;
+	atomic_inc(&conf->nr_pending);
+	if (atomic_read(&conf->barrier)) {
+		spin_lock_irq(&conf->resync_lock);
+		atomic_inc(&conf->nr_waiting);
+		atomic_dec(&conf->nr_pending);
 		/* Wait for the barrier to drop.
 		 * However if there are already pending
 		 * requests (preventing the barrier from
@@ -905,25 +906,23 @@ static void wait_barrier(struct r1conf *
 		 * that queue to get the nr_pending
 		 * count down.
 		 */
-		wait_event_lock_irq(conf->wait_barrier,
-				    !conf->array_frozen &&
-				    (!conf->barrier ||
-				    (conf->nr_pending &&
-				     current->bio_list &&
-				     !bio_list_empty(current->bio_list))),
-				    conf->resync_lock);
-		conf->nr_waiting--;
+		wait_event_lock_irq(
+			conf->wait_barrier,
+			!test_bit(R1FLAG_ARRAY_FROZEN, &conf->flags) &&
+			(atomic_read(&conf->barrier) ||
+			 (atomic_read(&conf->nr_pending) &&
+			 current->bio_list &&
+			 !bio_list_empty(current->bio_list))),
+			conf->resync_lock);
+		atomic_inc(&conf->nr_pending);
+		atomic_dec(&conf->nr_waiting);
+		spin_unlock_irq(&conf->resync_lock);
 	}
-	conf->nr_pending++;
-	spin_unlock_irq(&conf->resync_lock);
 }
 
 static void allow_barrier(struct r1conf *conf)
 {
-	unsigned long flags;
-	spin_lock_irqsave(&conf->resync_lock, flags);
-	conf->nr_pending--;
-	spin_unlock_irqrestore(&conf->resync_lock, flags);
+	atomic_dec(&conf->nr_pending);
 	wake_up(&conf->wait_barrier);
 }
 
@@ -940,21 +939,22 @@ static void freeze_array(struct r1conf *
 	 * must match the number of pending IOs (nr_pending) before
 	 * we continue.
 	 */
-	spin_lock_irq(&conf->resync_lock);
-	conf->array_frozen = 1;
-	wait_event_lock_irq_cmd(conf->wait_barrier,
-				conf->nr_pending == conf->nr_queued+extra,
-				conf->resync_lock,
-				flush_pending_writes(conf));
-	spin_unlock_irq(&conf->resync_lock);
+	if (!test_and_set_bit(R1FLAG_ARRAY_FROZEN, &conf->flags)) {
+		spin_lock_irq(&conf->resync_lock);
+		wait_event_lock_irq_cmd(conf->wait_barrier,
+				  atomic_read(&conf->nr_pending) ==
+					conf->nr_queued + extra,
+				  conf->resync_lock,
+				  flush_pending_writes(conf));
+		spin_unlock_irq(&conf->resync_lock);
+	}
 }
+
 static void unfreeze_array(struct r1conf *conf)
 {
 	/* reverse the effect of the freeze */
-	spin_lock_irq(&conf->resync_lock);
-	conf->array_frozen = 0;
-	wake_up(&conf->wait_barrier);
-	spin_unlock_irq(&conf->resync_lock);
+	if (test_and_set_bit(R1FLAG_ARRAY_FROZEN, &conf->flags))
+		wake_up(&conf->wait_barrier);
 }
 
 /* duplicate the data pages for behind I/O
@@ -1599,7 +1599,7 @@ static int raid1_add_disk(struct mddev *
 			 * if this was recently any drive of the array
 			 */
 			if (rdev->saved_raid_disk < 0)
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 			rcu_assign_pointer(p->rdev, rdev);
 			break;
 		}
@@ -1610,7 +1610,7 @@ static int raid1_add_disk(struct mddev *
 			set_bit(Replacement, &rdev->flags);
 			rdev->raid_disk = mirror;
 			err = 0;
-			conf->fullsync = 1;
+			set_bit(R1FLAG_FULLSYNC, &conf->flags);
 			rcu_assign_pointer(p[conf->raid_disks].rdev, rdev);
 			break;
 		}
@@ -2511,7 +2511,7 @@ static sector_t sync_request(struct mdde
 			bitmap_end_sync(mddev->bitmap, mddev->curr_resync,
 						&sync_blocks, 1);
 		else /* completed sync */
-			conf->fullsync = 0;
+			clear_bit(R1FLAG_FULLSYNC, &conf->flags);
 
 		bitmap_close_sync(mddev->bitmap);
 		close_sync(conf);
@@ -2526,7 +2526,7 @@ static sector_t sync_request(struct mdde
 	if (mddev->bitmap == NULL &&
 	    mddev->recovery_cp == MaxSector &&
 	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery) &&
-	    conf->fullsync == 0) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags)) {
 		*skipped = 1;
 		return max_sector - sector_nr;
 	}
@@ -2534,7 +2534,8 @@ static sector_t sync_request(struct mdde
 	 * This call the bitmap_start_sync doesn't actually record anything
 	 */
 	if (!bitmap_start_sync(mddev->bitmap, sector_nr, &sync_blocks, 1) &&
-	    !conf->fullsync && !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
+	    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
+	    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery)) {
 		/* We can skip this block, and probably several more */
 		*skipped = 1;
 		return sync_blocks;
@@ -2544,7 +2545,7 @@ static sector_t sync_request(struct mdde
 	 * and resync is going fast enough,
 	 * then let it though before starting on this new sync request.
 	 */
-	if (!go_faster && conf->nr_waiting)
+	if (!go_faster && atomic_read(&conf->nr_waiting))
 		msleep_interruptible(1000);
 
 	/* we are incrementing sector_nr below. To be safe, we check against
@@ -2710,7 +2711,7 @@ static sector_t sync_request(struct mdde
 		if (sync_blocks == 0) {
 			if (!bitmap_start_sync(mddev->bitmap, sector_nr,
 					       &sync_blocks, still_degraded) &&
-			    !conf->fullsync &&
+			    !test_bit(R1FLAG_FULLSYNC, &conf->flags) &&
 			    !test_bit(MD_RECOVERY_REQUESTED, &mddev->recovery))
 				break;
 			BUG_ON(sync_blocks < (PAGE_SIZE>>9));
@@ -2885,7 +2886,7 @@ static struct r1conf *setup_conf(struct
 			disk->head_position = 0;
 			if (disk->rdev &&
 			    (disk->rdev->saved_raid_disk < 0))
-				conf->fullsync = 1;
+				set_bit(R1FLAG_FULLSYNC, &conf->flags);
 		}
 	}
 
@@ -3200,7 +3201,7 @@ static void *raid1_takeover(struct mddev
 		conf = setup_conf(mddev);
 		if (!IS_ERR(conf))
 			/* Array must appear to be quiesced */
-			conf->array_frozen = 1;
+			set_bit(R1FLAG_ARRAY_FROZEN, &conf->flags);
 		return conf;
 	}
 	return ERR_PTR(-EINVAL);
--- a/drivers/md/raid1.h
+++ b/drivers/md/raid1.h
@@ -66,17 +66,16 @@ struct r1conf {
 	 */
 	wait_queue_head_t	wait_barrier;
 	spinlock_t		resync_lock;
-	int			nr_pending;
-	int			nr_waiting;
+	atomic_t		nr_pending;
+	atomic_t		nr_waiting;
 	int			nr_queued;
-	int			barrier;
-	int			array_frozen;
-
+	atomic_t		barrier;
+	unsigned long		flags;
+#define R1FLAG_ARRAY_FROZEN 0
 	/* Set to 1 if a full sync is needed, (fresh device added).
 	 * Cleared when a sync completes.
 	 */
-	int			fullsync;
-
+#define R1FLAG_FULLSYNC 1
 	/* When the same as mddev->recovery_disabled we don't allow
 	 * recovery to be attempted as we expect a read error.
 	 */
