From 24400e3acbf7120a6e016590d56c2d97d220ece4 Mon Sep 17 00:00:00 2001
From: Waiman Long <Waiman.Long@hpe.com>
Date: Mon, 14 Dec 2015 13:20:50 -0500
Subject: [PATCH 2/2] sched/fair: Disable tg load_avg/runnable_avg update for root_task_group
Git-commit: aa0b7ae06387d40a988ce16a189082dee6e570bc
Patch-mainline: v4.5-rc1
References: bnc#960227

Currently, the __update_cfs_rq_tg_load_contrib() and
__update_tg_runnable_avg() functions attempts to update the tg's
load_avg/runnable_avg values even for root_task_group where their
values will never be used. This patch will disable the update when
the given task group is the root_task_group.

This patch is a backport of upstream commit:

	aa0b7ae06387d40a988ce16a189082dee6e570bc
	sched/fair: Disable the task group load_avg update for the
	root_task_group

Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
Acked-by: Mike Galbraith <mgalbraith@suse.de>
---
 kernel/sched/fair.c |   11 ++++++++++-
 1 file changed, 10 insertions(+), 1 deletion(-)

--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -2164,7 +2164,10 @@ static inline void __update_cfs_rq_tg_lo
 	tg_contrib = cfs_rq->runnable_load_avg + cfs_rq->blocked_load_avg;
 	tg_contrib -= cfs_rq->tg_load_contrib;
 
-	if (!tg_contrib)
+	/*
+	 * No need to update load_avg for root_task_group as it is not used.
+	 */
+	if (!tg_contrib || (tg == &root_task_group))
 		return;
 
 	if (force_update || abs(tg_contrib) > cfs_rq->tg_load_contrib / 8) {
@@ -2183,6 +2186,12 @@ static inline void __update_tg_runnable_
 	struct task_group *tg = cfs_rq->tg;
 	long contrib;
 
+	/*
+	 * No need to update runnable_avg for root_task_group as it is not used.
+	 */
+	if (tg == &root_task_group)
+		return;
+
 	/* The fraction of a cpu used by this cfs_rq */
 	contrib = div_u64((u64)sa->runnable_avg_sum << NICE_0_SHIFT,
 			  sa->runnable_avg_period + 1);
