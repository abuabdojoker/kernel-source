From 86b6c3ae49f0a4b3b2942902f1f41c9d6fa69605 Mon Sep 17 00:00:00 2001
From: Mel Gorman <mgorman@suse.de>
Date: Tue, 24 Nov 2015 08:34:56 +0000
Subject: [PATCH] sched: Put expensive runtime debugging checks under a
 separate Kconfig entry

References: Scheduler performance
Patch-mainline: No, upstream would need a more comprehensive approach

SCHED_DEBUG is meant to have low overhead but there are some expensive checks
made in scheduler hot paths. This patch places the expensive checks under a
separate kconfig entry CONFIG_SCHED_DEBUG_PARANOID. Mainline would require a
more comprehensive approach that separated out tunables, basic debugging and
expensive debugging. Ultimately the intent would be to remove the fast path
overhead so for now, just get rid of the worst criminal. Ideally there would
also be work done to reduce the overhead of schedstats which are currently
far heavier than they should be for a feature that is almost always enabled.

The results are not fantastic but they're promising enough to be worthwhile.

UMA machine, 8 CPU threads (machine is called ivy and not available in orthos)

hackbench-pipes
                          3.12.50               3.12.50
               sle12-sp1-20151124    schedparanoid-v1r1
Amean    1       0.0513 (  0.00%)      0.0487 (  5.01%)
Amean    3       0.1151 (  0.00%)      0.1161 ( -0.87%)
Amean    5       0.1781 (  0.00%)      0.1741 (  2.25%)
Amean    7       0.2349 (  0.00%)      0.2357 ( -0.36%)
Amean    12      0.3846 (  0.00%)      0.3789 (  1.49%)
Amean    18      0.5523 (  0.00%)      0.5559 ( -0.65%)
Amean    24      0.7634 (  0.00%)      0.7466 (  2.21%)
Amean    30      0.9539 (  0.00%)      0.9299 (  2.52%)
Amean    32      1.0073 (  0.00%)      0.9896 (  1.76%)

Marginal gains but better than nothing.

netperf UDP_STREAM
                                  3.12.50               3.12.50
                       sle12-sp1-20151124    schedparanoid-v1r1
Hmean    send-64         326.27 (  0.00%)      344.64 (  5.63%)
Hmean    send-128        658.43 (  0.00%)      677.38 (  2.88%)
Hmean    send-256       1274.13 (  0.00%)     1321.17 (  3.69%)
Hmean    send-1024      5093.00 (  0.00%)     5282.93 (  3.73%)
Hmean    send-2048     10292.26 (  0.00%)    10701.79 (  3.98%)
Hmean    send-3312     15621.69 (  0.00%)    16302.35 (  4.36%)
Hmean    send-4096     18403.51 (  0.00%)    18992.00 (  3.20%)
Hmean    send-8192     30567.48 (  0.00%)    30570.24 (  0.01%)
Hmean    send-16384    47536.78 (  0.00%)    47490.39 ( -0.10%)
Hmean    recv-64         326.26 (  0.00%)      344.53 (  5.60%)
Hmean    recv-128        658.37 (  0.00%)      677.25 (  2.87%)
Hmean    recv-256       1274.10 (  0.00%)     1321.09 (  3.69%)
Hmean    recv-1024      5092.69 (  0.00%)     5282.73 (  3.73%)
Hmean    recv-2048     10291.90 (  0.00%)    10701.18 (  3.98%)
Hmean    recv-3312     15621.02 (  0.00%)    16301.63 (  4.36%)
Hmean    recv-4096     18402.77 (  0.00%)    18991.46 (  3.20%)
Hmean    recv-8192     30566.59 (  0.00%)    30568.75 (  0.01%)
Hmean    recv-16384    47535.22 (  0.00%)    47489.31 ( -0.10%)

Very roughly a 3% to 5% gain for sub-page buffer sizes.

Other scheduler micro-benchmarks on this machine showed nothing
worthwhile. TCP_STREAM and tbench4 showed nothing useful on this machine.

On a 4-node NUMA machine with 48 CPu threads

hackbench-pipes
                           3.12.50               3.12.50
                sle12-sp1-20151124    schedparanoid-v1r1
Amean    1        0.2153 (  0.00%)      0.1853 ( 13.93%)
Amean    4        0.3674 (  0.00%)      0.3987 ( -8.51%)
Amean    7        0.4644 (  0.00%)      0.4744 ( -2.15%)
Amean    12       0.5420 (  0.00%)      0.5066 (  6.54%)
Amean    21       0.6564 (  0.00%)      0.6154 (  6.25%)
Amean    30       0.8280 (  0.00%)      0.8070 (  2.54%)
Amean    48       1.2254 (  0.00%)      1.2006 (  2.03%)
Amean    79       2.0470 (  0.00%)      2.0507 ( -0.18%)
Amean    110      2.5300 (  0.00%)      2.6311 ( -4.00%)
Amean    141      3.4317 (  0.00%)      3.2677 (  4.78%)
Amean    172      4.2597 (  0.00%)      4.1120 (  3.47%)
Amean    192      4.4621 (  0.00%)      4.2340 (  5.11%)

It's a little less consistent but there are more gains than there are losses.

pipetest
                               3.12.50               3.12.50
                    sle12-sp1-20151124    schedparanoid-v1r1
Min         Time       10.89 (  0.00%)       10.76 (  1.19%)
1st-qrtle   Time       11.05 (  0.00%)       10.88 (  1.54%)
2nd-qrtle   Time       11.12 (  0.00%)       10.90 (  1.98%)
3rd-qrtle   Time       11.17 (  0.00%)       10.93 (  2.15%)
Max-90%     Time       11.20 (  0.00%)       10.95 (  2.23%)
Max-93%     Time       11.22 (  0.00%)       10.96 (  2.32%)
Max-95%     Time       11.22 (  0.00%)       10.97 (  2.23%)
Max-99%     Time       11.26 (  0.00%)       11.01 (  2.22%)
Max         Time       11.26 (  0.00%)       11.02 (  2.13%)
Mean        Time       11.10 (  0.00%)       10.90 (  1.82%)
Best99%Mean Time       11.10 (  0.00%)       10.90 (  1.82%)
Best95%Mean Time       11.10 (  0.00%)       10.90 (  1.80%)
Best90%Mean Time       11.09 (  0.00%)       10.89 (  1.78%)
Best50%Mean Time       11.04 (  0.00%)       10.87 (  1.50%)
Best10%Mean Time       10.94 (  0.00%)       10.83 (  0.98%)
Best5%Mean  Time       10.92 (  0.00%)       10.81 (  0.99%)
Best1%Mean  Time       10.90 (  0.00%)       10.78 (  1.10%)

pipetest on the UMA machine was flat but it showed a small gain here. Netperf
also showed a small gain

netperf-udp
                                  3.12.50               3.12.50
                       sle12-sp1-20151124    schedparanoid-v1r1
Hmean    send-64         326.27 (  0.00%)      344.64 (  5.63%)
Hmean    send-128        658.43 (  0.00%)      677.38 (  2.88%)
Hmean    send-256       1274.13 (  0.00%)     1321.17 (  3.69%)
Hmean    send-1024      5093.00 (  0.00%)     5282.93 (  3.73%)
Hmean    send-2048     10292.26 (  0.00%)    10701.79 (  3.98%)
Hmean    send-3312     15621.69 (  0.00%)    16302.35 (  4.36%)
Hmean    send-4096     18403.51 (  0.00%)    18992.00 (  3.20%)
Hmean    send-8192     30567.48 (  0.00%)    30570.24 (  0.01%)
Hmean    send-16384    47536.78 (  0.00%)    47490.39 ( -0.10%)
Hmean    recv-64         326.26 (  0.00%)      344.53 (  5.60%)
Hmean    recv-128        658.37 (  0.00%)      677.25 (  2.87%)
Hmean    recv-256       1274.10 (  0.00%)     1321.09 (  3.69%)
Hmean    recv-1024      5092.69 (  0.00%)     5282.73 (  3.73%)
Hmean    recv-2048     10291.90 (  0.00%)    10701.18 (  3.98%)
Hmean    recv-3312     15621.02 (  0.00%)    16301.63 (  4.36%)
Hmean    recv-4096     18402.77 (  0.00%)    18991.46 (  3.20%)
Hmean    recv-8192     30566.59 (  0.00%)    30568.75 (  0.01%)
Hmean    recv-16384    47535.22 (  0.00%)    47489.31 ( -0.10%)

Signed-off-by: Mel Gorman <mgorman@suse.de>
---
 kernel/sched/core.c  | 2 +-
 kernel/sched/debug.c | 2 ++
 kernel/sched/fair.c  | 4 ++--
 kernel/sched/rt.c    | 2 +-
 kernel/sched/sched.h | 1 +
 lib/Kconfig.debug    | 9 +++++++++
 6 files changed, 16 insertions(+), 4 deletions(-)

diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index b48de7eb3541..1a7ba1ac1b8a 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -999,7 +999,7 @@ void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags)
 #ifdef CONFIG_SMP
 void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 {
-#ifdef CONFIG_SCHED_DEBUG
+#ifdef CONFIG_SCHED_DEBUG_PARANOID
 	/*
 	 * We should never call set_task_cpu() on a blocked task,
 	 * ttwu() will sort out the placement.
diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index ff912554237f..7bcf7486cfbd 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -209,8 +209,10 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	spread0 = min_vruntime - rq0_min_vruntime;
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "spread0",
 			SPLIT_NS(spread0));
+#ifdef CONFIG_SCHED_DEBUG_PARANOID
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_spread_over",
 			cfs_rq->nr_spread_over);
+#endif
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_SMP
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 234a91f2c91a..69ea54236107 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -263,7 +263,7 @@ static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
 
 static inline struct task_struct *task_of(struct sched_entity *se)
 {
-#ifdef CONFIG_SCHED_DEBUG
+#ifdef CONFIG_SCHED_DEBUG_PARANOID
 	WARN_ON_ONCE(!entity_is_task(se));
 #endif
 	return container_of(se, struct task_struct, se);
@@ -2822,7 +2822,7 @@ static void enqueue_sleeper(struct cfs_rq *cfs_rq, struct sched_entity *se)
 
 static void check_spread(struct cfs_rq *cfs_rq, struct sched_entity *se)
 {
-#ifdef CONFIG_SCHED_DEBUG
+#ifdef CONFIG_SCHED_DEBUG_PARANOID
 	s64 d = se->vruntime - cfs_rq->min_vruntime;
 
 	if (d < 0)
diff --git a/kernel/sched/rt.c b/kernel/sched/rt.c
index d0d1d9b7dc7d..ad2462f9ab66 100644
--- a/kernel/sched/rt.c
+++ b/kernel/sched/rt.c
@@ -98,7 +98,7 @@ static void destroy_rt_bandwidth(struct rt_bandwidth *rt_b)
 
 static inline struct task_struct *rt_task_of(struct sched_rt_entity *rt_se)
 {
-#ifdef CONFIG_SCHED_DEBUG
+#ifdef CONFIG_SCHED_DEBUG_PARANOID
 	WARN_ON_ONCE(!rt_entity_is_task(rt_se));
 #endif
 	return container_of(rt_se, struct task_struct, rt);
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 89a7f3fb0420..3a23405d5051 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -267,6 +267,7 @@ struct cfs_rq {
 	struct sched_entity *curr, *next, *last, *skip;
 
 #ifdef	CONFIG_SCHED_DEBUG
+	/* Unused due to SCHED_DEBUG_PARANOID but still required by KABI */
 	unsigned int nr_spread_over;
 #endif
 
diff --git a/lib/Kconfig.debug b/lib/Kconfig.debug
index 094f3152ec2b..38e9605638df 100644
--- a/lib/Kconfig.debug
+++ b/lib/Kconfig.debug
@@ -761,6 +761,15 @@ config SCHED_DEBUG
 	  that can help debug the scheduler. The runtime overhead of this
 	  option is minimal.
 
+config SCHED_DEBUG_PARANOID
+	bool "Add additional paranoid debugging checks to scheduler"
+	depends on DEBUG_KERNEL && SCHED_DEBUG && PROC_FS
+	default n
+	help
+	  If you say Y here, additional debugging checks will be made in
+	  some of the scheduler hot paths. The runtime overhead is small
+	  but it is measurable.
+
 config SCHEDSTATS
 	bool "Collect scheduler statistics"
 	depends on DEBUG_KERNEL && PROC_FS
