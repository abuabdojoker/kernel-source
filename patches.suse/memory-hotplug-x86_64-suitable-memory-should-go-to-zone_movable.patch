From: Wang Nan <wangnan0@huawei.com>
Subject: memory-hotplug: x86_64: suitable memory should go to ZONE_MOVABLE

References: fate#316483
Patch-mainline: v3.17-rc1
Git-commit: 9bfc41138525c51955cd35cbca56f8cd757a73f8

This patch introduces zone_for_memory() to arch_add_memory() on x86_64 to
ensure new, higher memory added into ZONE_MOVABLE if movable zone has
already setup.

Signed-off-by: Wang Nan <wangnan0@huawei.com>
Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
Signed-off-by: Mel Gorman <mgorman@suse.de>
---

 arch/x86/mm/init_64.c |    3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff -puN arch/x86/mm/init_64.c~memory-hotplug-x86_64-suitable-memory-should-go-to-zone_movable arch/x86/mm/init_64.c
--- a/arch/x86/mm/init_64.c~memory-hotplug-x86_64-suitable-memory-should-go-to-zone_movable
+++ a/arch/x86/mm/init_64.c
@@ -702,7 +702,8 @@ static void  update_end_of_memory_vars(u
 int arch_add_memory(int nid, u64 start, u64 size)
 {
 	struct pglist_data *pgdat = NODE_DATA(nid);
-	struct zone *zone = pgdat->node_zones + ZONE_NORMAL;
+	struct zone *zone = pgdat->node_zones +
+		zone_for_memory(nid, start, size, ZONE_NORMAL);
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	int ret;
